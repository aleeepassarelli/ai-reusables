# -----------------------------------------------------------------
# üß© Atomic Architecture - Docker Compose
# -----------------------------------------------------------------
#
# Este arquivo define os servi√ßos de infraestrutura (Stack C).
#
# --- Como usar os Perfis (Profiles) ---
#
# 1. Iniciar APENAS os servi√ßos base (Neo4j, Redis):
#    (Obrigat√≥rio para a 'api_mcp' funcionar)
#    docker-compose up -d
#
# 2. Iniciar TUDO (Base + AI Local + Monitoramento):
#    (Recomendado para desenvolvimento completo)
#    docker-compose --profile dev up -d
#
# -----------------------------------------------------------------

version: '3.8'

services:
  # ----------------------------------------------------
  # --- SERVI√áOS BASE (Sempre ligados) ---
  # ----------------------------------------------------
  
  # 1. Neo4j (Mem√≥ria de Grafo - Camada 1)
  # (Conforme solicitado: neo4j:5.26)
  neo4j:
    image: neo4j:5.26
    container_name: atomic_neo4j
    ports:
      - "7474:7474" # Interface Web HTTP
      - "7687:7687" # Porta Bolt (usada pelo 'graphiti-core' e 'neo4j' driver)
    environment:
      # Defina uma senha forte em um .env
      - NEO4J_AUTH=neo4j/sua-senha-segura-aqui
    volumes:
      - neo4j_data:/data
    healthcheck:
      test: ["CMD-SHELL", "wget -O /dev/null http://localhost:7474 || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 5

  # 2. Redis (Cache de Sess√£o e Filas - Camada 1)
  # (Conforme solicitado: redis:latest)
  redis:
    image: redis:latest
    container_name: atomic_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # ----------------------------------------------------
  # --- PERFIL "dev": AI Local & Monitoramento ---
  # ----------------------------------------------------
  
  # 3. Ollama (Servidor de IA Local - Camada 3)
  # (Conforme solicitado: ollama/ollama)
  ollama:
    image: ollama/ollama
    container_name: atomic_ollama
    # Ativa o perfil 'dev' para este servi√ßo
    profiles: ["dev"]
    ports:
      - "11434:11434"
    volumes:
      # Persiste os modelos de IA baixados
      - ollama_data:/root/.ollama
    # Configura√ß√£o para acesso √† GPU (se dispon√≠vel)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  # 4. Prometheus (Coleta de M√©tricas)
  # (Conforme solicitado: prom/prometheus)
  prometheus:
    image: prom/prometheus
    container_name: atomic_prometheus
    profiles: ["dev"]
    ports:
      - "9090:9090"
    volumes:
      # Requer um 'prometheus.yml' de configura√ß√£o
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command: --config.file=/etc/prometheus/prometheus.yml

  # 5. Grafana (Visualiza√ß√£o de M√©tricas)
  # (Conforme solicitado: grafana/grafana)
  grafana:
    image: grafana/grafana
    container_name: atomic_grafana
    profiles: ["dev"]
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus

# ----------------------------------------------------
# --- Volumes Persistentes ---
# ----------------------------------------------------
volumes:
  neo4j_data:
  redis_data:
  ollama_data:
  prometheus_data:
  grafana_data:

  ‚ö†Ô∏è A√ß√µes Necess√°rias (Importante!)
Este docker-compose.yml n√£o funcionar√° sozinho. Ele tem duas depend√™ncias externas que voc√™ precisa criar:

Configura√ß√£o do Prometheus: O servi√ßo prometheus falhar√° ao iniciar se n√£o encontrar o arquivo de configura√ß√£o. Crie uma pasta config na raiz do projeto e, dentro dela, um arquivo prometheus.yml:

Crie o arquivo: config/prometheus.yml

YAML

# config/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Adicione aqui seus outros servi√ßos (ex: 'atomic_backend')
  # quando eles estiverem prontos para serem monitorados.
  #- job_name: 'atomic_backend'
  #  static_configs:
  #    - targets: ['atomic_backend:8000'] 
Puxar o Modelo Jan (Ollama): O servi√ßo ollama inicia vazio. Ap√≥s inici√°-lo (docker-compose --profile dev up -d), voc√™ precisa executar este comando para baixar o jan-v1-4B (o seu agent_mcp):

Bash

docker exec -it atomic_ollama ollama pull janhq/jan-v1-4b
